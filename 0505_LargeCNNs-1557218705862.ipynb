{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6KNf9CD2ytqH"
   },
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BDKI47gsW1Ii"
   },
   "source": [
    "1. Loading datasets - Transforming images\n",
    "2. VGG-16 with modification to network head\n",
    "3. Using pre-trained models\n",
    "4. Storing intermediate models\n",
    "5. Resnet\n",
    "6. Inception v3\n",
    "7. Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rbZpqiiiylAg"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1NBprN3Ry2NE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hGzpz3oE1Ikz"
   },
   "source": [
    "## Dataset, transforms, and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9c57fNA5Wsi"
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fvsHorMPzISb"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or corrupted. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-953e49571af9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n\u001b[0m\u001b[0;32m      2\u001b[0m                                         \u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                         transform=transform_train)\n\u001b[0;32m      4\u001b[0m testset = torchvision.datasets.CIFAR10(root='./data', train=False, \n\u001b[0;32m      5\u001b[0m                                         \u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Sree\\lib\\site-packages\\torchvision\\datasets\\cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             raise RuntimeError('Dataset not found or corrupted.' +\n\u001b[0m\u001b[0;32m     69\u001b[0m                                ' You can use download=True to download it')\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dataset not found or corrupted. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                        download=False, \n",
    "                                        transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, \n",
    "                                        download=False, \n",
    "                                        transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0dOpKtiAG4s"
   },
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTNj3LQY4eTS"
   },
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KfLwRIXH08tg"
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KEOz-75x1NGu"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "print(images[1].shape)\n",
    "print(labels[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oBeIwYC1N3c"
   },
   "outputs": [],
   "source": [
    "def imshow(img, title):\n",
    "    npimg = img.numpy() / 2 + 0.5\n",
    "    plt.figure(figsize=(batch_size, 1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cfk-SYLY1Sbl"
   },
   "outputs": [],
   "source": [
    "def show_batch_images(dataloader):\n",
    "    images, labels = next(iter(dataloader))\n",
    "    img = torchvision.utils.make_grid(images)\n",
    "    imshow(img, title=[str(x.item()) for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_Zi_s3p1htN"
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    show_batch_images(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1E-pnTG97DDz"
   },
   "source": [
    "## Creating VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_QMSp0gEJLow"
   },
   "source": [
    "https://pytorch.org/docs/master/_modules/torchvision/models/vgg.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ra33PbJS28P3"
   },
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvte5SSA7G7r"
   },
   "outputs": [],
   "source": [
    "vgg = models.vgg16_bn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7w-p73Tz9aZ_"
   },
   "outputs": [],
   "source": [
    "print(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N1bQdxQz-Dil"
   },
   "outputs": [],
   "source": [
    "print(vgg.features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CX9PohSB-1Dx"
   },
   "outputs": [],
   "source": [
    "print(vgg.classifier[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koDRbWi8_ApT"
   },
   "outputs": [],
   "source": [
    "final_in_features = vgg.classifier[6].in_features\n",
    "mod_classifier = list(vgg.classifier.children())[:-1]\n",
    "mod_classifier.extend([nn.Linear(final_in_features, num_classes)])\n",
    "print(mod_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_vHjXi1j_glv"
   },
   "outputs": [],
   "source": [
    "vgg.classifier = nn.Sequential(*mod_classifier)\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DVR1FiuJ-BzX"
   },
   "source": [
    "### Train CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKnlGE1q7JtN"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtRHmDs_BvZG"
   },
   "outputs": [],
   "source": [
    "def evaluation(dataloader, model):\n",
    "    total, correct = 0, 0\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "htVdEliECDsz"
   },
   "outputs": [],
   "source": [
    "vgg = vgg.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(vgg.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xSWnZJxjBbwW"
   },
   "outputs": [],
   "source": [
    "loss_epoch_arr = []\n",
    "max_epochs = 1\n",
    "\n",
    "n_iters = np.ceil(50000/batch_size)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        outputs = vgg(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        del inputs, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Iteration: %d/%d, Loss: %0.2f' % (i, n_iters, loss.item()))\n",
    "        \n",
    "    loss_epoch_arr.append(loss.item())\n",
    "        \n",
    "    print('Epoch: %d/%d, Test acc: %0.2f, Train acc: %0.2f' % (\n",
    "        epoch, max_epochs, \n",
    "        evaluation(testloader, vgg), evaluation(trainloader, vgg)))\n",
    "    \n",
    "plt.plot(loss_epoch_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bh4gREC-IGFB"
   },
   "source": [
    "### Freeze layers of Convolutional Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3aD1lt1qcJOU"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FzniE5sKKlnY"
   },
   "outputs": [],
   "source": [
    "vgg = models.vgg16_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzaHEHaczpWS"
   },
   "outputs": [],
   "source": [
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFAhAvahzrRh"
   },
   "outputs": [],
   "source": [
    "final_in_features = vgg.classifier[6].in_features\n",
    "vgg.classifier[6] = nn.Linear(final_in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqXCJEiK0Z2V"
   },
   "outputs": [],
   "source": [
    "for param in vgg.parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lH-99npmPRUk"
   },
   "outputs": [],
   "source": [
    "vgg = vgg.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(vgg.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5HuWwxaxIMTV"
   },
   "outputs": [],
   "source": [
    "loss_epoch_arr = []\n",
    "max_epochs = 1\n",
    "\n",
    "n_iters = np.ceil(50000/batch_size)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        outputs = vgg(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Iteration: %d/%d, Loss: %0.2f' % (i, n_iters, loss.item()))\n",
    "            \n",
    "        del inputs, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    loss_epoch_arr.append(loss.item())\n",
    "        \n",
    "    print('Epoch: %d/%d, Test acc: %0.2f, Train acc: %0.2f' % (\n",
    "        epoch, max_epochs, \n",
    "        evaluation(testloader, vgg), evaluation(trainloader, vgg)))\n",
    "    \n",
    "    \n",
    "plt.plot(loss_epoch_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AeacSwlg5p2r"
   },
   "source": [
    "### With model copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4-dZMeUTpAxC"
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wf0qM_-PlXl-"
   },
   "outputs": [],
   "source": [
    "loss_epoch_arr = []\n",
    "max_epochs = 1\n",
    "\n",
    "min_loss = 1000\n",
    "\n",
    "n_iters = np.ceil(50000/batch_size)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        outputs = vgg(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if min_loss > loss.item():\n",
    "            min_loss = loss.item()\n",
    "            best_model = copy.deepcopy(vgg.state_dict())\n",
    "            print('Min loss %0.2f' % min_loss)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Iteration: %d/%d, Loss: %0.2f' % (i, n_iters, loss.item()))\n",
    "            \n",
    "        del inputs, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    loss_epoch_arr.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2GAOmBytwZ9"
   },
   "outputs": [],
   "source": [
    "vgg.load_state_dict(best_model)\n",
    "print(evaluation(trainloader, vgg), evaluation(testloader, vgg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "83zNw_MLFBhd"
   },
   "source": [
    "## ResNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJHZ2ibFGYfG"
   },
   "source": [
    "https://pytorch.org/docs/master/_modules/torchvision/models/resnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gYZJSHEKFDfS"
   },
   "outputs": [],
   "source": [
    "resnet = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMUoM7ToFFeK"
   },
   "outputs": [],
   "source": [
    "print(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1KOHDdtFoK5"
   },
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LrL2ZlhrGwFL"
   },
   "outputs": [],
   "source": [
    "in_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "53REdyaaG8ap"
   },
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQTZ3X_pG_tT"
   },
   "outputs": [],
   "source": [
    "resnet = resnet.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(resnet.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rklYsln-Hcpj"
   },
   "outputs": [],
   "source": [
    "loss_epoch_arr = []\n",
    "max_epochs = 4\n",
    "\n",
    "min_loss = 1000\n",
    "\n",
    "n_iters = np.ceil(50000/batch_size)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        outputs = resnet(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if min_loss > loss.item():\n",
    "            min_loss = loss.item()\n",
    "            best_model = copy.deepcopy(resnet.state_dict())\n",
    "            print('Min loss %0.2f' % min_loss)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Iteration: %d/%d, Loss: %0.2f' % (i, n_iters, loss.item()))\n",
    "            \n",
    "        del inputs, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    loss_epoch_arr.append(loss.item())\n",
    "        \n",
    "    print('Epoch: %d/%d, Test acc: %0.2f, Train acc: %0.2f' % (\n",
    "        epoch, max_epochs, \n",
    "        evaluation(testloader, resnet), evaluation(trainloader, resnet)))\n",
    "    \n",
    "    \n",
    "plt.plot(loss_epoch_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbetTwcrIMnQ"
   },
   "outputs": [],
   "source": [
    "resnet.load_state_dict(best_model)\n",
    "print(evaluation(trainloader, resnet), evaluation(testloader, resnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "asFT3q7vdDbd"
   },
   "source": [
    "## Inception Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-gBu2zCGWE8"
   },
   "source": [
    "https://pytorch.org/docs/master/_modules/torchvision/models/inception.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rpAr08aGIOle"
   },
   "outputs": [],
   "source": [
    "inception = models.inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TM_ACRUHdMfi"
   },
   "outputs": [],
   "source": [
    "print(inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nbu0JobP6ea9"
   },
   "outputs": [],
   "source": [
    "for param in inception.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j31it2qX5Nfs"
   },
   "outputs": [],
   "source": [
    "aux_in_features = inception.AuxLogits.fc.in_features\n",
    "inception.AuxLogits.fc = nn.Linear(aux_in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-M6nGOh6Xg2"
   },
   "outputs": [],
   "source": [
    "for param in inception.parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pOI1Au2A6l9X"
   },
   "outputs": [],
   "source": [
    "in_features = inception.fc.in_features\n",
    "inception.fc = nn.Linear(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sF5pPlUS7I-7"
   },
   "outputs": [],
   "source": [
    "for param in inception.parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMSMZa_j7JXG"
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(299), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(299), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYVy8C0H7vL7"
   },
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                        download=True, \n",
    "                                        transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, \n",
    "                                        download=True, \n",
    "                                        transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tkUGBe3x7zld"
   },
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeKDmSV98QfZ"
   },
   "outputs": [],
   "source": [
    "inception = inception.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(inception.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ou_Q_JwK_fqn"
   },
   "outputs": [],
   "source": [
    "def evaluation_inception(dataloader, model):\n",
    "    total, correct = 0, 0\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs, aux_outputs = model(inputs)\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VqmnOJqP75AK"
   },
   "outputs": [],
   "source": [
    "loss_epoch_arr = []\n",
    "max_epochs = 1\n",
    "\n",
    "min_loss = 1000\n",
    "\n",
    "n_iters = np.ceil(50000/batch_size)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        outputs, aux_outputs = inception(inputs)\n",
    "        loss = loss_fn(outputs, labels) + 0.3 * loss_fn(aux_outputs, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if min_loss > loss.item():\n",
    "            min_loss = loss.item()\n",
    "            best_model = copy.deepcopy(inception.state_dict())\n",
    "            print('Min loss %0.2f' % min_loss)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Iteration: %d/%d, Loss: %0.2f' % (i, n_iters, loss.item()))\n",
    "            \n",
    "        del inputs, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    loss_epoch_arr.append(loss.item())\n",
    "        \n",
    "    print('Epoch: %d/%d, Test acc: %0.2f, Train acc: %0.2f' % (\n",
    "        epoch, max_epochs, \n",
    "        evaluation_inception(testloader, inception), \n",
    "        evaluation_inception(trainloader, inception)))\n",
    "    \n",
    "    \n",
    "plt.plot(loss_epoch_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfcUcxaL8T_U"
   },
   "outputs": [],
   "source": [
    "inception.load_state_dict(best_model)\n",
    "print(evaluation_inception(trainloader, inception), evaluation_inception(testloader, inception))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EVxdZeh_JVVK"
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ONIJxTuqJW1W"
   },
   "source": [
    "1. Structure the above code into a series of functions and then call each model\n",
    "\n",
    "2. Try out different hyperparameter combinations and try to achieve published results on different networks\n",
    "\n",
    "3. Try out the CIFAR100 and STL10 datasets\n",
    "\n",
    "4. Try out another model - SqueezeNet\n",
    "\n",
    "5. Try training multiple layers and not just the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bL9x2x9Dw9L5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DVR1FiuJ-BzX"
   ],
   "name": "0505_LargeCNNs.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
